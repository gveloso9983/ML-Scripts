# -*- coding: utf-8 -*-
"""Cópia de Cópia de MLP_lei_TEST

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ON_gIdcL3QlUoZRKbbry1zLmRYtp8Com
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import plot_model
from scipy import stats

#for replicability purposes
tf.random.set_seed(91195003)
#for an easy reset backend session state
tf.keras.backend.clear_session()

"""################
#1ª PARTE - Load, prepare and split data
################

*Load data*
"""

df_raw = pd.read_csv("dataset_balanced.csv")
df_raw.shape

"""##Tratamento de Outliers"""

print(df_raw.info())

df_raw.describe()

"""Identifying Outliers with Interquartile Range (IQR)"""

# Q1 = df_raw.quantile(0.25)
# Q3 = df_raw.quantile(0.75)
# IQR = Q3 - Q1
# print(IQR)

#print(df_raw < (Q1 - 1.5 * IQR)) or (df_raw > (Q3 + 1.5 * IQR))

"""Este modo tratamento de outliers elimina muitos registos. Como tal decidimos tratar melhor a idade, escolhendo um intervalo de idades entre 20 e os 80 anos (porque a media = 57 e o desvio padrao é de 20 daí 57-20, 57+20 ("margem de segurança"))

Selecionar registos com idades superiores a 20 e inferiores a 80
"""

df_raw = df_raw.loc[(df_raw['Idade'] >20) & (df_raw['Idade']<80)]
df_raw

df_raw['Idade'] = pd.qcut(df_raw['Idade'], q=6, precision=0, labels=[0,1,2,3,4,5])
df_raw

"""##*Normalize data*

Normalize only the 'Idade' column
"""

#min_max_scaler = preprocessing.MinMaxScaler()
#df_min_max_scaled = df_raw.copy()
#column = 'Idade'
#df_raw[column] = (df_raw[column] - df_raw[column].min()) / (df_raw[column].max() - df_raw[column].min())    
#np_scaled = min_max_scaler.fit_transform(df_age)
#cols = df_raw.columns
#df_normalized = pd.DataFrame(df_raw, columns = cols)

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_raw)
cols = df_raw.columns
df_normalized = pd.DataFrame(np_scaled, columns = cols)
df_normalized

"""*Drop irrelevant features*"""

df_data = df_normalized.drop(columns='UTI')
df_label = df_normalized['UTI']

# df_data = df_data.drop(columns=['CS_GESTANT','CO_PAIS','SURTO_SG','OUTRO_SIN','RAIOX_RES','CLASSI_FIN','FADIGA','PERD_PALA'])
df_data = df_data.drop(columns=['CO_PAIS','PUERPERA','SIND_DOWN','HEMATOLOGI','HEPATICA','TP_IDADE'])

"""Split data"""

X_train, X_test, y_train, y_test = train_test_split(df_data, df_label, test_size=0.3, random_state=50)

print("Train data : ",X_train.shape," Train label: ",y_train.shape," Test data: ",X_test.shape," Test label: ",y_test.shape)

X_validation = X_train[-20000:]
y_validation = y_train[-20000:]
X_train = X_train[:-20000]
y_train = y_train[:-20000]

print(X_validation.shape)
print(X_train.shape)
print(y_validation.shape)
print(y_train.shape)

"""**Multi Layer Perceptron**"""

#hyperparameters
epochs = 6
batch_size = 32
learning_rate = 1e-2
output_neurons = 1
number_features = len(df_data.columns)

"""*Model*"""

model = Sequential()

model.add(Dense(32,input_dim=number_features,activation='relu'))

model.add(Dense(64,activation='relu'))

model.add(Dense(128,activation='relu'))

model.add(Dense(256,activation='relu'))

model.add(Dropout(0.2))

model.add(Dense(output_neurons,activation='sigmoid'))

plot_model(model, show_shapes=True, show_layer_names=True)

"""**Compile & Fit**


"""

model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_validation,y_validation))

"""**Evaluation**

loss, accuracy datset test
"""

y_test

loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)

print("Model loss: %.2f, Accuracy: %.2f" % ((loss*100),(accuracy*100)))

"""Experiencia"""

from sklearn.neural_network import MLPClassifier

clf = MLPClassifier(max_iter=100)


#MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1, solver='adam', validation_fraction=0.1)
parameter_space = {
    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],
    'activation': ['tanh', 'relu'],
    'solver': ['lbfgs','sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}

from sklearn.model_selection import GridSearchCV

clf = GridSearchCV(clf, parameter_space, n_jobs=-1, cv=3)
clf.fit(X_train, y_train)

# Best paramete set
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

y_true, y_pred = y_test , clf.predict(X_test)

from sklearn.metrics import classification_report
print('Results on the test set:')
print(classification_report(y_true, y_pred))

clf.predict(X_test)

clf.score(X_test,y_test)