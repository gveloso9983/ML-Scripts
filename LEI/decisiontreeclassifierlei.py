# -*- coding: utf-8 -*-
"""DecisionTreeClassifierLEI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18SuGeY8HwP1Zfb7CgLLj9tdGACApmdzv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics

#from google.colab import drive
#drive.mount('/content/drive')



dataset = pd.read_csv('dataset_balanced.csv')
dataset.shape

"""####################################################################################################################################################################################################

**Normalizar dados**
"""

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(dataset)
cols = dataset.columns
df_normalized = pd.DataFrame(np_scaled, columns = cols)
df_normalized

"""***Feature selection***"""

df_data = df_normalized.drop(columns='UTI')
df_label = df_normalized['UTI']
df_data.columns

"""*Teste R^2*"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Feature extraction
test = SelectKBest(score_func=chi2)
fit = test.fit(df_data, df_label)

"""R^2"""

# Summarize scores
np.set_printoptions(precision=3,suppress=True)
print(fit.scores_)

3

"""P valores:"""

np.set_printoptions(precision=3)
print(fit.pvalues_)

df_data.columns

features = fit.transform(df_data)
# Summarize selected features
print(features)

"""*Teste Pearson correlation*"""

import seaborn as sns

#Using Pearson Correlation
plt.figure(figsize=(40,40))
cor = df_normalized.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()
#Correlation with output variable
cor_target = abs(df_normalized['UTI'])
#Selecting highly correlated features
relevant_features = cor_target[cor_target>0.5]
relevant_features

"""***mutual info classifier***

Estimate mutual information for a discrete target variable.

Mutual information (MI) [1] between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.

The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in [2] and [3]. Both methods are based on the idea originally proposed in [4].
"""

from sklearn.feature_selection import mutual_info_classif

threshold = 15  # the number of most relevant features
high_score_features = []
feature_scores = mutual_info_classif(df_data, df_label, random_state=0)
for score, f_name in sorted(zip(feature_scores, df_data.columns), reverse=True)[:threshold]:
        print(f_name, score)
        high_score_features.append(f_name)
df_wine_norm_mic = df_data[high_score_features]
print(df_wine_norm_mic.columns)

"""**Drop atributos irrelevantes**"""

df_data.columns

df_data = df_data.drop(columns=['CS_GESTANT','CO_PAIS','SURTO_SG','OUTRO_SIN','RAIOX_RES','CLASSI_FIN'])
df_data.shape

"""**Data division**"""

df_data.shape

X_train, X_test, y_train, y_test = train_test_split(df_data, df_label, test_size=0.3, random_state=50)

"""**Decision Tree Classifier**"""

classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""**Logistic Regression classifier**

cross validation : K-fold
"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from numpy import mean
from numpy import std

# prepare the cross-validation procedure
cv = KFold(n_splits=10, random_state=1, shuffle=True)

"""Building model"""

clf = LogisticRegression(random_state=0, max_iter=5000).fit(X_train, y_train)

"""Predict"""

clf.predict(X_test)

"""Confusion matrix"""

print(confusion_matrix(y_test, y_pred))

"""Accuracy Score without cross-validation"""

clf.score(X_test, y_test)

"""Evaluate model with cross-validation"""

scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

"""**Support Vector Machine(SVM)**"""

from sklearn import svm

"""SVM com cross-validation"""

clf_svm_cv =svm.SVC(kernel='linear', C=1)

clf_svm_cv.fit(X_train,y_train)

scores = cross_val_score(clf_svm_cv, X_test, y_test, cv=5)
scores

print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

"""**Tunning**"""

svc = svm.SVC(kernel='linear')
#C_s = np.logspace(-10, 0, 10)
C_s=[1,4,7,10]

scores = list()
scores_std = list()
for C in C_s:
    svc.C = C
    this_scores = cross_val_score(svc, X_test, y_test, n_jobs=1)
    scores.append(np.mean(this_scores))
    scores_std.append(np.std(this_scores))

# Do the plotting
import matplotlib.pyplot as plt
plt.figure()
plt.semilogx(C_s, scores, 'g', label="CV_score")
plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--', label="CV_score + Std")
plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'r--', label="CV_score - Std")
locs, labels = plt.yticks()
plt.yticks(locs, list(map(lambda x: "%g" % x, locs)))
plt.ylabel('CV score')
plt.xlabel('Parameter C')
plt.legend()
plt.ylim(0.68, 0.7)
plt.show()

"""SVM sem cross-validation ou parametros"""

clf_svm = svm.SVC()

X_train.shape

clf_svm.fit(X_train, y_train)

y_predito = clf_svm.predict(X_test)

from sklearn import metrics

metrics.accuracy_score(y_test, y_predito)

metrics.confusion_matrix(y_test,y_predito)