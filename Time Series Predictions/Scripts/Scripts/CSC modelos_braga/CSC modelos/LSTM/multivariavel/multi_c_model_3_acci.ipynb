{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_c_model_3_acci.ipynb","provenance":[{"file_id":"1lbGPN0WkFVGPY-Njk3NKHde7vW05K9Ec","timestamp":1621364682853},{"file_id":"1xMfdhhY4RH5_3jQXz2eKNsQxPNHmFZfW","timestamp":1621364653621},{"file_id":"1IcsDklI9fXx51smuGHGfTtMWpV-zbKEO","timestamp":1620867097679},{"file_id":"1E4z4wgPDgVWmISuP3GHEnTJycgAJ7r-F","timestamp":1620815610877},{"file_id":"1Cjz2LsJ4MOwKrWqM9-kHQysDocNI3rWk","timestamp":1620815599783},{"file_id":"1dJlE2Fi0FD7qG4SpaqSrqzCDfUD38edx","timestamp":1620815322050}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"eZx8k29epf1z"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","import matplotlib.pyplot as plt\n","\n","#for replicability purposes\n","tf.random.set_seed(91195003)\n","#for an easy reset backend session state\n","tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVOimP-I-uoU"},"source":["n_variate = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJeT5IxR4y_r"},"source":["\n","#Load dataset\n","def load_dataset(path=r'dataset_test_v3.csv'):\n","  return pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwT1idGM4y1T"},"source":["#split data into training and validation sets\n","def split_data(training, perc=10):\n","  train_idx = np.arange(0, int(len(training)*(100-perc)/100))\n","  val_idx = np.arange(int(len(training)*(100-perc)/100+1), len(training))\n","  return train_idx, val_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPqJhFvWptro"},"source":["def prepare_data(df):\n","  # df_aux = df.drop(columns=['nr_ruas_afetadas', 'comprimento_fila_metros', 'demora_fila_segundos'], inplace=False)\n","  df_aux = df\n","  df_aux[\"data\"] = pd.to_datetime(df_aux[\"data\"])\n","  df_aux = df_aux.sort_values('data')\n","  df_aux = df_aux.set_index('data')\n","  return df_aux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TImupJ1qKqCL"},"source":["def data_normalization(df, norm_range=(-1, 1)):\n","    #[-1, 1] for LSTM due to the internal use of tanh by the memory cell\n","    scaler = MinMaxScaler(feature_range=norm_range)\n","    df[['nr_ruas_afetadas']] = scaler.fit_transform(df[['nr_ruas_afetadas']])\n","    df[['comprimento_fila_metros']] = scaler.fit_transform(df[['comprimento_fila_metros']])\n","    df[['demora_fila_segundos']] = scaler.fit_transform(df[['demora_fila_segundos']])\n","    df[['Nr_acidentes']] = scaler.fit_transform(df[['Nr_acidentes']])\n","\n","    return scaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jmEehmE1PxG"},"source":["#plot learning curve\n","def plot_learning_curves(history, epochs):\n","  #accuracies and losses\n","  #dict_keys(['loss', 'mae', 'rmse', 'val_loss', 'val_mae', 'val_rmse'])\n","  loss=history.history['loss']\n","  val_loss=history.history['val_loss']\n","  mae=history.history['mae']\n","  val_mae=history.history['val_mae']\n","  rmse=history.history['rmse']\n","  val_rmse=history.history['val_rmse']\n","  epochs_range = range(epochs)\n","  #creating figure\n","  plt.figure(figsize=(8,8))\n","  plt.subplot(1,2,2)\n","  plt.plot(epochs_range,loss,label='Training Loss')\n","  plt.plot(epochs_range,val_loss,label='Validation Loss')\n","  # plt.plot(epochs_range,mae,label='Training MAE')\n","  # plt.plot(epochs_range,val_mae,label='Validation MAE')\n","  # plt.plot(epochs_range,rmse,label='Training RMSE')\n","  # plt.plot(epochs_range,val_rmse,label='Validation RMSE')\n","  plt.legend(loc='upper right')\n","  plt.title('Training/Validation Loss')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xMNt9uhN1A5X"},"source":["#Plot time series data\n","def plot_confirmed_cases(data):\n","  plt.figure(figsize=(8,6))\n","  plt.plot(range(len(data)), data)\n","  plt.title('Confirmed Cases of COVID-19')\n","  plt.ylabel('Cases')\n","  plt.xlabel('Days')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HyufbCYptpH"},"source":["#Preparing the dataset for the LSTM\n","def to_supervised(df, timesteps):\n","  data = df.values\n","  X, y = list(), list()\n","  #iterate over the training set to create X and y\n","  dataset_size = len(data)\n","  for curr_pos in range(dataset_size):\n","    #end of the input sequence is the current position + the number of timesteps of the input sequence\n","    input_index = curr_pos + timesteps\n","    #end of the labels corresponds to the end of the input sequence + 1\n","    label_index = input_index + 1\n","    #if we have enough data for this sequence\n","    if label_index < dataset_size:\n","      X.append(data[curr_pos:input_index, :])\n","      y.append(data[input_index:label_index, 0])\n","  #using np.float32 for GPU performance\n","  return np.array(X).astype('float32'), np.array(y).astype('float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsLQU54X1I9t"},"source":["#Building the model\n","def rmse(y_true, y_pred):\n","  return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5JUf24TupyZg"},"source":["# 1 LSTM, 1 LSTM, 1 Densa, 1 Dropout, 1 Densa\n","def build_model(timesteps, features, h_neurons=64, activation='tanh'):\n","    model = tf.keras.models.Sequential()\n","    model.add(tf.keras.layers.LSTM(h_neurons, activation=activation, input_shape=(timesteps, features), return_sequences=True))\n","    #Add a new layer\n","    model.add(tf.keras.layers.LSTM(32, activation=activation ,return_sequences=False))\n","    #\n","    model.add(tf.keras.layers.Dense(h_neurons, activation=activation))\n","    model.add(tf.keras.layers.Dropout(0.2))\n","    model.add(tf.keras.layers.Dense(n_variate, activation='linear'))\n","\n","    #model summary (and save it as PNG)\n","    tf.keras.utils.plot_model(model, 'accidents_model.png', show_shapes=True)\n","    model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fWl4YLypyWR"},"source":["#Compiling and fit the model\n","def compile_and_fit(model, epochs, batch_size):\n","  #compile\n","  model.compile(loss = rmse, optimizer = tf.keras.optimizers.Adam(), metrics = ['mae', rmse])\n","  #fit\n","  hist_list = list()\n","  loss_list = list()\n","\n","  #callback\n","  #saving in Keras HDF5 (or h5), a binary data format\n","  callbacks = [tf.keras.callbacks.ModelCheckpoint(\n","        filepath='my_model_{epoch}_{val_loss:.3f}.h5',#path where to save model\n","        save_best_only=True,#overwrite the current checkpoint if and only if\n","        monitor='val_loss',#the val_loss score has improved\n","        save_weights_only=False,#if True, only the weigths are saved\n","        verbose=1,#verbosity mode\n","        period=5 #save ony at the fifth epoch (5 em 5 epocas) \n","        )#,\n","    #interrupt training if loss stops improving for over 2 epochs\n","    #tf.keras.callbacks.EarlyStopping(patience=9, monitor='cost')\n","    ]\n","\n","  #Time Series Cross Validator\n","  tscv = TimeSeriesSplit(n_splits=cv_splits)\n","  for train_index, test_index in tscv.split(X):\n","    train_idx, val_idx = split_data(train_index, perc=10) #further split into training and validation sets\n","    #build data\n","    X_train, y_train = X[train_idx], y[train_idx]\n","    X_val, y_val = X[val_idx], y[val_idx]\n","    X_test, y_test = X[test_index], y[test_index]\n","\n","    #print(\"x_val::::\",X_val,\"\\n\",\"y_val:\",y_val,\"\\n\")\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=epochs, batch_size=batch_size, shuffle=False, callbacks=callbacks)\n","    metrics = model.evaluate(X_test, y_test)\n","\n","\n","    plot_learning_curves(history, epochs)\n","    hist_list.append(history)\n","\n","\n","  return model, hist_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Sv6JlRppySv"},"source":["#Main Execution\n","#the dataframes\n","df_raw = load_dataset()\n","df_data = prepare_data(df_raw)\n","df = df_data.copy()\n","\n","scaler = data_normalization(df) #scaling data to [-1, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LU2y0-4bqA94"},"source":["#Recursive Multi-Step Forecast!!!\n","def forecast(model, df, timesteps, multisteps, scaler):\n","  input_seq = np.array(df[-timesteps:].values) #getting the last sequence of known value\n","  inp = input_seq\n","  #print(\"Input_seq: \",inp)\n","  forecasts = list()\n","\n","  #multisteps tells us how many iterations we want to perform, i.e., how many days we want to predict\n","  for step in range(1, multisteps+1):\n","    inp = inp.reshape(1,timesteps,n_variate)\n","    yhat = model.predict(inp) #dá o valor predito normalizado\n","    yhat_desnormalized = scaler.inverse_transform(yhat) #dá valor predito desnormalizado\n","    forecasts.append(yhat_desnormalized) #adicionar previsao à lista final de previsões\n","    list_yhat = [yhat[0][i] for i in range(len(yhat[0]))]\n","    #preparar novo input para fazer previsão para o dia seguinte\n","    inp= np.append(inp[0],[list_yhat],axis=0) #adiciona previsão recente ao input\n","    inp = inp[-timesteps:] #vai ao input buscar os ultimos timesteps registados\n","  return forecasts\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_xOdNdF135D"},"source":["def plot_forecast(data, forecasts):\n","\n","  plt.figure(figsize=(14,6))\n","  plt.plot(range(len(data)), data['Nr_acidentes'], color='green', label='True value')\n","  plt.plot(range(len(data)-1, len(data)+len(forecasts)-1), forecasts, color='red', label='Forecasts')\n","  plt.title('Nr de Acidentes em Braga')\n","  plt.ylabel('Value')\n","  plt.xlabel('Days')\n","  plt.legend()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Od6i2gQP-EaZ"},"source":["# Tunning"]},{"cell_type":"code","metadata":{"id":"2bebumjE9_7Z"},"source":["'''tunning_dict = {               \n","                1: {'timesteps' : 7, 'multisteps' : 100, 'cv_splits': 10, 'epochs' : 10,  'batch_size' : 4 },\n","                2: {'timesteps' : 7, 'multisteps' : 100, 'cv_splits' : 10, 'epochs' : 50,  'batch_size' : 4 },\n","                3: {'timesteps' : 7, 'multisteps' : 100, 'cv_splits' : 10, 'epochs' : 100,  'batch_size' : 4 },\n","\n","                #\n","                4: {'timesteps' : 14, 'multisteps' : 100, 'cv_splits': 10, 'epochs' : 10,  'batch_size' : 2 },\n","                5: {'timesteps' : 14, 'multisteps' : 100, 'cv_splits' : 10, 'epochs' : 50,  'batch_size' : 2 },\n","                6: {'timesteps' : 14, 'multisteps' : 100, 'cv_splits' : 10, 'epochs' : 100,  'batch_size' : 2 }#,\n","                #\n","                #7: {'timesteps' : 30, 'multisteps' : 100, 'cv_splits': 5, 'epochs' : 10,  'batch_size' : 1 },\n","                #8: {'timesteps' : 30, 'multisteps' : 100, 'cv_splits' : 5, 'epochs' : 50,  'batch_size' : 1 },\n","                #9: {'timesteps' : 30, 'multisteps' : 100, 'cv_splits' : 5, 'epochs' : 100,  'batch_size' : 1 }\n","\n","                }'''\n","\n","tunning_dict = {               \n","                1: {'timesteps' : 7, 'multisteps' : 15, 'cv_splits': 3, 'epochs' : 10,  'batch_size' : 5 },\n","                2: {'timesteps' : 7, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 50,  'batch_size' : 5 },\n","                3: {'timesteps' : 7, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 100,  'batch_size' : 5 },\n","\n","                \n","                4: {'timesteps' : 8, 'multisteps' : 15, 'cv_splits': 3, 'epochs' : 60,  'batch_size' : 5 },\n","                5: {'timesteps' : 8, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 90,  'batch_size' : 5 },\n","                6: {'timesteps' : 8, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 120,  'batch_size' : 5 },\n","                \n","                7: {'timesteps' : 10, 'multisteps' : 15, 'cv_splits': 3, 'epochs' : 50,  'batch_size' : 8 },\n","                8: {'timesteps' : 10, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 90,  'batch_size' : 8 },\n","                9: {'timesteps' : 10, 'multisteps' : 15, 'cv_splits' : 3, 'epochs' : 120,  'batch_size' : 8 }\n","\n","}\n","\n","# record da history de cada modelo\n","record = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12UVI_Xymt_KQv_zgBBNyVFXLPVN7JwcQ"},"id":"bYN09hNN-Iiw","executionInfo":{"status":"ok","timestamp":1621733606341,"user_tz":-60,"elapsed":865901,"user":{"displayName":"Hugo Nogueira","photoUrl":"","userId":"00187793632493684428"}},"outputId":"8b4dada2-81dd-467f-f95c-ec79cca4ea46"},"source":["for t in tunning_dict:\n","  #print(record[r])\n","  # fitting the model\n","  timesteps = tunning_dict[t]['timesteps']\n","  epochs = tunning_dict[t]['epochs']\n","  batch_size= tunning_dict[t]['batch_size']\n","  multisteps= tunning_dict[t]['multisteps']\n","  cv_splits = tunning_dict[t]['cv_splits']\n","  #print(timesteps,epochs,batch_size,cv_splits)\n","\n","  X, y = to_supervised(df, timesteps)\n","\n","  model = build_model(timesteps, n_variate)\n","  model, history = compile_and_fit(model, epochs, batch_size)\n","  #print(\"df: \",df.shape,\" timesteps\",timesteps,\" multisteps \",multisteps)\n","  forecasts = forecast(model, df, timesteps, multisteps, scaler)\n","\n","  print(forecasts)\n","\n","  prev = []\n","\n","  #plot do valor previsto da ação de Open\n","  for f in forecasts:\n","    prev.append(f[0][0])\n","\n","  print(prev)\n","  plot_forecast(df_raw, prev)\n","\n","  #Scorer\n","  \n","\n","  record[t] = history"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"KWvKPj311_Cd"},"source":["id_tunning = 1\n","id_split =1\n","\n","final_dict = {}\n","\n","for r in record:\n","#print(tunning_dict[1]['epochs'])\n","  loss = []\n","  mae =[]\n","  rmse = []\n","  val_loss = []\n","  val_mae = []\n","  val_rmse = []\n","\n","  for h in record[r]:\n","    #print(\"Tunning ID:  \",id_tunning,\" Split ID: \",id_split)\n","    #plot_learning_curves(h, tunning_dict[id_tunning]['epochs'])\n","    #['loss', 'mae', 'rmse', 'val_loss', 'val_mae', 'val_rmse']\n","    #print(\"loss: \",sum(h.history['loss'])/len(h.history['loss']),\" MAE: \",sum(h.history['mae'])/len(h.history['mae']),\" RMSE: \",sum(h.history['rmse'])/len(h.history['rmse']),\" VAL_LOSS: \",sum(h.history['val_loss'])/len(h.history['val_loss']),\" VAL_MAE: \",sum(h.history['val_mae'])/len(h.history['val_mae']),\" VAL_RMSE: \",sum(h.history['val_rmse'])/len(h.history['val_rmse']))\n","    loss.append(sum(h.history['loss'])/len(h.history['loss']))\n","    mae.append(sum(h.history['mae'])/len(h.history['mae']))\n","    rmse.append(sum(h.history['rmse'])/len(h.history['rmse']))\n","    val_loss.append(sum(h.history['val_loss'])/len(h.history['val_loss']))\n","    val_mae.append(sum(h.history['val_mae'])/len(h.history['val_mae']))\n","    val_rmse.append(sum(h.history['val_rmse'])/len(h.history['val_rmse']))\n","    id_split+=1\n","  id_split=1\n","  \n","  final_dict[id_tunning]=[sum(loss)/len(loss), sum(mae)/len(mae),sum(rmse)/len(rmse),sum(val_loss)/len(val_loss),sum(val_mae)/len(val_mae), sum(val_rmse)/len(val_rmse)]\n","\n","  id_tunning=id_tunning+1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHY-qstM2HOs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621733606344,"user_tz":-60,"elapsed":27,"user":{"displayName":"Hugo Nogueira","photoUrl":"","userId":"00187793632493684428"}},"outputId":"9bc18b47-2bd6-47d9-c6aa-81ca536a38dd"},"source":["for f in final_dict:\n","  print(\"Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\")\n","  print(\"ID tunning: \",f, \" Valores: \",final_dict[f],\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  1  Valores:  [0.28106738328933717, 0.2381310443083445, 0.28206588625907897, 0.3567950293421745, 0.3130866418282191, 0.350332802037398] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  2  Valores:  [0.2182699501514435, 0.18156339257955553, 0.2187146054704984, 0.3018172734975815, 0.25800713777542117, 0.3048234659433365] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  3  Valores:  [0.1760185104608536, 0.14515243202447892, 0.17643043465912342, 0.354314310948054, 0.3030062304933866, 0.365667393853267] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  4  Valores:  [0.20318079698416924, 0.17212001788947318, 0.20349723200003308, 0.3129715657896466, 0.2578532858027352, 0.3201579882038964] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  5  Valores:  [0.17660848939860307, 0.1486960637624617, 0.17739204419431862, 0.33459422086124063, 0.2839496064517233, 0.34412320763976484] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  6  Valores:  [0.15230272241557638, 0.12833484907944995, 0.15264005959033966, 0.41518003915747004, 0.3504006908171707, 0.42851774336563214] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  7  Valores:  [0.23393643349409107, 0.19702306419610974, 0.23255297591288882, 0.3454805587728818, 0.28653892050186797, 0.32238965004682535] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  8  Valores:  [0.19350146769925403, 0.1607158256073793, 0.1922151842602977, 0.3398625824186537, 0.28911464159135464, 0.31513254498993915] \n","\n","Loss | MAE | RMSE | VAL_LOSS | VAL_MAE | VAL_RMSE\n","ID tunning:  9  Valores:  [0.1775013069311778, 0.14743251287274892, 0.1766727857291698, 0.3618365337451299, 0.3090523882044686, 0.3438612760768997] \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"XgaElTKdelmL","executionInfo":{"status":"ok","timestamp":1621733606345,"user_tz":-60,"elapsed":21,"user":{"displayName":"Hugo Nogueira","photoUrl":"","userId":"00187793632493684428"}},"outputId":"2bf8aa98-4c32-407b-9531-fe7760ab8920"},"source":["pd.DataFrame.from_dict(final_dict, orient='index',columns=['Loss', 'MAE', 'RMSE', 'VAL_LOSS', 'VAL_MAE', 'VAL_RMSE'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Loss</th>\n","      <th>MAE</th>\n","      <th>RMSE</th>\n","      <th>VAL_LOSS</th>\n","      <th>VAL_MAE</th>\n","      <th>VAL_RMSE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.281067</td>\n","      <td>0.238131</td>\n","      <td>0.282066</td>\n","      <td>0.356795</td>\n","      <td>0.313087</td>\n","      <td>0.350333</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.218270</td>\n","      <td>0.181563</td>\n","      <td>0.218715</td>\n","      <td>0.301817</td>\n","      <td>0.258007</td>\n","      <td>0.304823</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.176019</td>\n","      <td>0.145152</td>\n","      <td>0.176430</td>\n","      <td>0.354314</td>\n","      <td>0.303006</td>\n","      <td>0.365667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.203181</td>\n","      <td>0.172120</td>\n","      <td>0.203497</td>\n","      <td>0.312972</td>\n","      <td>0.257853</td>\n","      <td>0.320158</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.176608</td>\n","      <td>0.148696</td>\n","      <td>0.177392</td>\n","      <td>0.334594</td>\n","      <td>0.283950</td>\n","      <td>0.344123</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.152303</td>\n","      <td>0.128335</td>\n","      <td>0.152640</td>\n","      <td>0.415180</td>\n","      <td>0.350401</td>\n","      <td>0.428518</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.233936</td>\n","      <td>0.197023</td>\n","      <td>0.232553</td>\n","      <td>0.345481</td>\n","      <td>0.286539</td>\n","      <td>0.322390</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.193501</td>\n","      <td>0.160716</td>\n","      <td>0.192215</td>\n","      <td>0.339863</td>\n","      <td>0.289115</td>\n","      <td>0.315133</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.177501</td>\n","      <td>0.147433</td>\n","      <td>0.176673</td>\n","      <td>0.361837</td>\n","      <td>0.309052</td>\n","      <td>0.343861</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Loss       MAE      RMSE  VAL_LOSS   VAL_MAE  VAL_RMSE\n","1  0.281067  0.238131  0.282066  0.356795  0.313087  0.350333\n","2  0.218270  0.181563  0.218715  0.301817  0.258007  0.304823\n","3  0.176019  0.145152  0.176430  0.354314  0.303006  0.365667\n","4  0.203181  0.172120  0.203497  0.312972  0.257853  0.320158\n","5  0.176608  0.148696  0.177392  0.334594  0.283950  0.344123\n","6  0.152303  0.128335  0.152640  0.415180  0.350401  0.428518\n","7  0.233936  0.197023  0.232553  0.345481  0.286539  0.322390\n","8  0.193501  0.160716  0.192215  0.339863  0.289115  0.315133\n","9  0.177501  0.147433  0.176673  0.361837  0.309052  0.343861"]},"metadata":{"tags":[]},"execution_count":20}]}]}